{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169b6d83-e2e1-4054-b727-1eac1bb8b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (838.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.4/838.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103203cd-36d3-4cf0-9449-7f4548fa58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available num GPU 1\n",
      "GPU 0: NVIDIA RTX A6000\n",
      "  - Total G mem: 47.4381103515625 GB\n",
      "  - #Cores: 84\n"
     ]
    }
   ],
   "source": [
    "from utils import honesty_function_dataset # TODO, define our own dataset layout later\n",
    "from hooked_transformer import hooked_transformer\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "# transformers Model save path\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/data/xxr230000/huggingface_model\"  \n",
    "# set gpus\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"available num GPU\", torch.cuda.device_count())\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"  - Total G mem:\", torch.cuda.get_device_properties(i).total_memory / (1024 ** 3), \"GB\")\n",
    "        print(\"  - #Cores:\", torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU.\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1750060d-bf5c-4a69-8bca-0fb8f553819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9053ea48827f4f6a97492f80a9652c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/people/cs/x/xxr230000/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "This function print all the modules in this model,\n",
      "since different model will have different architecture and different module name......\n",
      "please select the module you want to hook manually after check the forward function and the layout of the modules\n",
      "----------------------------------------------------------------------------------------\n",
      "root:<lm_head,Linear>\n",
      "root:<model,LlamaModel>\n",
      "root/model:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model:<norm,LlamaRMSNorm>\n",
      "root/model:<layers,ModuleList>\n",
      "root/model:<embed_tokens,Embedding>\n",
      "root/model/layers:<39,LlamaDecoderLayer>\n",
      "root/model/layers:<38,LlamaDecoderLayer>\n",
      "root/model/layers:<37,LlamaDecoderLayer>\n",
      "root/model/layers:<36,LlamaDecoderLayer>\n",
      "root/model/layers:<35,LlamaDecoderLayer>\n",
      "root/model/layers:<34,LlamaDecoderLayer>\n",
      "root/model/layers:<33,LlamaDecoderLayer>\n",
      "root/model/layers:<32,LlamaDecoderLayer>\n",
      "root/model/layers:<31,LlamaDecoderLayer>\n",
      "root/model/layers:<30,LlamaDecoderLayer>\n",
      "root/model/layers:<29,LlamaDecoderLayer>\n",
      "root/model/layers:<28,LlamaDecoderLayer>\n",
      "root/model/layers:<27,LlamaDecoderLayer>\n",
      "root/model/layers:<26,LlamaDecoderLayer>\n",
      "root/model/layers:<25,LlamaDecoderLayer>\n",
      "root/model/layers:<24,LlamaDecoderLayer>\n",
      "root/model/layers:<23,LlamaDecoderLayer>\n",
      "root/model/layers:<22,LlamaDecoderLayer>\n",
      "root/model/layers:<21,LlamaDecoderLayer>\n",
      "root/model/layers:<20,LlamaDecoderLayer>\n",
      "root/model/layers:<19,LlamaDecoderLayer>\n",
      "root/model/layers:<18,LlamaDecoderLayer>\n",
      "root/model/layers:<17,LlamaDecoderLayer>\n",
      "root/model/layers:<16,LlamaDecoderLayer>\n",
      "root/model/layers:<15,LlamaDecoderLayer>\n",
      "root/model/layers:<14,LlamaDecoderLayer>\n",
      "root/model/layers:<13,LlamaDecoderLayer>\n",
      "root/model/layers:<12,LlamaDecoderLayer>\n",
      "root/model/layers:<11,LlamaDecoderLayer>\n",
      "root/model/layers:<10,LlamaDecoderLayer>\n",
      "root/model/layers:<9,LlamaDecoderLayer>\n",
      "root/model/layers:<8,LlamaDecoderLayer>\n",
      "root/model/layers:<7,LlamaDecoderLayer>\n",
      "root/model/layers:<6,LlamaDecoderLayer>\n",
      "root/model/layers:<5,LlamaDecoderLayer>\n",
      "root/model/layers:<4,LlamaDecoderLayer>\n",
      "root/model/layers:<3,LlamaDecoderLayer>\n",
      "root/model/layers:<2,LlamaDecoderLayer>\n",
      "root/model/layers:<1,LlamaDecoderLayer>\n",
      "root/model/layers:<0,LlamaDecoderLayer>\n",
      "root/model/layers/0:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/0:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/0:<mlp,LlamaMLP>\n",
      "root/model/layers/0:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/0/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/0/self_attn:<o_proj,Linear>\n",
      "root/model/layers/0/self_attn:<v_proj,Linear>\n",
      "root/model/layers/0/self_attn:<k_proj,Linear>\n",
      "root/model/layers/0/self_attn:<q_proj,Linear>\n",
      "root/model/layers/0/mlp:<act_fn,SiLU>\n",
      "root/model/layers/0/mlp:<down_proj,Linear>\n",
      "root/model/layers/0/mlp:<up_proj,Linear>\n",
      "root/model/layers/0/mlp:<gate_proj,Linear>\n",
      "root/model/layers/1:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/1:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/1:<mlp,LlamaMLP>\n",
      "root/model/layers/1:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/1/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/1/self_attn:<o_proj,Linear>\n",
      "root/model/layers/1/self_attn:<v_proj,Linear>\n",
      "root/model/layers/1/self_attn:<k_proj,Linear>\n",
      "root/model/layers/1/self_attn:<q_proj,Linear>\n",
      "root/model/layers/1/mlp:<act_fn,SiLU>\n",
      "root/model/layers/1/mlp:<down_proj,Linear>\n",
      "root/model/layers/1/mlp:<up_proj,Linear>\n",
      "root/model/layers/1/mlp:<gate_proj,Linear>\n",
      "root/model/layers/2:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/2:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/2:<mlp,LlamaMLP>\n",
      "root/model/layers/2:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/2/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/2/self_attn:<o_proj,Linear>\n",
      "root/model/layers/2/self_attn:<v_proj,Linear>\n",
      "root/model/layers/2/self_attn:<k_proj,Linear>\n",
      "root/model/layers/2/self_attn:<q_proj,Linear>\n",
      "root/model/layers/2/mlp:<act_fn,SiLU>\n",
      "root/model/layers/2/mlp:<down_proj,Linear>\n",
      "root/model/layers/2/mlp:<up_proj,Linear>\n",
      "root/model/layers/2/mlp:<gate_proj,Linear>\n",
      "root/model/layers/3:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/3:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/3:<mlp,LlamaMLP>\n",
      "root/model/layers/3:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/3/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/3/self_attn:<o_proj,Linear>\n",
      "root/model/layers/3/self_attn:<v_proj,Linear>\n",
      "root/model/layers/3/self_attn:<k_proj,Linear>\n",
      "root/model/layers/3/self_attn:<q_proj,Linear>\n",
      "root/model/layers/3/mlp:<act_fn,SiLU>\n",
      "root/model/layers/3/mlp:<down_proj,Linear>\n",
      "root/model/layers/3/mlp:<up_proj,Linear>\n",
      "root/model/layers/3/mlp:<gate_proj,Linear>\n",
      "root/model/layers/4:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/4:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/4:<mlp,LlamaMLP>\n",
      "root/model/layers/4:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/4/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/4/self_attn:<o_proj,Linear>\n",
      "root/model/layers/4/self_attn:<v_proj,Linear>\n",
      "root/model/layers/4/self_attn:<k_proj,Linear>\n",
      "root/model/layers/4/self_attn:<q_proj,Linear>\n",
      "root/model/layers/4/mlp:<act_fn,SiLU>\n",
      "root/model/layers/4/mlp:<down_proj,Linear>\n",
      "root/model/layers/4/mlp:<up_proj,Linear>\n",
      "root/model/layers/4/mlp:<gate_proj,Linear>\n",
      "root/model/layers/5:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/5:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/5:<mlp,LlamaMLP>\n",
      "root/model/layers/5:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/5/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/5/self_attn:<o_proj,Linear>\n",
      "root/model/layers/5/self_attn:<v_proj,Linear>\n",
      "root/model/layers/5/self_attn:<k_proj,Linear>\n",
      "root/model/layers/5/self_attn:<q_proj,Linear>\n",
      "root/model/layers/5/mlp:<act_fn,SiLU>\n",
      "root/model/layers/5/mlp:<down_proj,Linear>\n",
      "root/model/layers/5/mlp:<up_proj,Linear>\n",
      "root/model/layers/5/mlp:<gate_proj,Linear>\n",
      "root/model/layers/6:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/6:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/6:<mlp,LlamaMLP>\n",
      "root/model/layers/6:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/6/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/6/self_attn:<o_proj,Linear>\n",
      "root/model/layers/6/self_attn:<v_proj,Linear>\n",
      "root/model/layers/6/self_attn:<k_proj,Linear>\n",
      "root/model/layers/6/self_attn:<q_proj,Linear>\n",
      "root/model/layers/6/mlp:<act_fn,SiLU>\n",
      "root/model/layers/6/mlp:<down_proj,Linear>\n",
      "root/model/layers/6/mlp:<up_proj,Linear>\n",
      "root/model/layers/6/mlp:<gate_proj,Linear>\n",
      "root/model/layers/7:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/7:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/7:<mlp,LlamaMLP>\n",
      "root/model/layers/7:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/7/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/7/self_attn:<o_proj,Linear>\n",
      "root/model/layers/7/self_attn:<v_proj,Linear>\n",
      "root/model/layers/7/self_attn:<k_proj,Linear>\n",
      "root/model/layers/7/self_attn:<q_proj,Linear>\n",
      "root/model/layers/7/mlp:<act_fn,SiLU>\n",
      "root/model/layers/7/mlp:<down_proj,Linear>\n",
      "root/model/layers/7/mlp:<up_proj,Linear>\n",
      "root/model/layers/7/mlp:<gate_proj,Linear>\n",
      "root/model/layers/8:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/8:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/8:<mlp,LlamaMLP>\n",
      "root/model/layers/8:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/8/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/8/self_attn:<o_proj,Linear>\n",
      "root/model/layers/8/self_attn:<v_proj,Linear>\n",
      "root/model/layers/8/self_attn:<k_proj,Linear>\n",
      "root/model/layers/8/self_attn:<q_proj,Linear>\n",
      "root/model/layers/8/mlp:<act_fn,SiLU>\n",
      "root/model/layers/8/mlp:<down_proj,Linear>\n",
      "root/model/layers/8/mlp:<up_proj,Linear>\n",
      "root/model/layers/8/mlp:<gate_proj,Linear>\n",
      "root/model/layers/9:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/9:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/9:<mlp,LlamaMLP>\n",
      "root/model/layers/9:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/9/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/9/self_attn:<o_proj,Linear>\n",
      "root/model/layers/9/self_attn:<v_proj,Linear>\n",
      "root/model/layers/9/self_attn:<k_proj,Linear>\n",
      "root/model/layers/9/self_attn:<q_proj,Linear>\n",
      "root/model/layers/9/mlp:<act_fn,SiLU>\n",
      "root/model/layers/9/mlp:<down_proj,Linear>\n",
      "root/model/layers/9/mlp:<up_proj,Linear>\n",
      "root/model/layers/9/mlp:<gate_proj,Linear>\n",
      "root/model/layers/10:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/10:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/10:<mlp,LlamaMLP>\n",
      "root/model/layers/10:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/10/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/10/self_attn:<o_proj,Linear>\n",
      "root/model/layers/10/self_attn:<v_proj,Linear>\n",
      "root/model/layers/10/self_attn:<k_proj,Linear>\n",
      "root/model/layers/10/self_attn:<q_proj,Linear>\n",
      "root/model/layers/10/mlp:<act_fn,SiLU>\n",
      "root/model/layers/10/mlp:<down_proj,Linear>\n",
      "root/model/layers/10/mlp:<up_proj,Linear>\n",
      "root/model/layers/10/mlp:<gate_proj,Linear>\n",
      "root/model/layers/11:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/11:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/11:<mlp,LlamaMLP>\n",
      "root/model/layers/11:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/11/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/11/self_attn:<o_proj,Linear>\n",
      "root/model/layers/11/self_attn:<v_proj,Linear>\n",
      "root/model/layers/11/self_attn:<k_proj,Linear>\n",
      "root/model/layers/11/self_attn:<q_proj,Linear>\n",
      "root/model/layers/11/mlp:<act_fn,SiLU>\n",
      "root/model/layers/11/mlp:<down_proj,Linear>\n",
      "root/model/layers/11/mlp:<up_proj,Linear>\n",
      "root/model/layers/11/mlp:<gate_proj,Linear>\n",
      "root/model/layers/12:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/12:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/12:<mlp,LlamaMLP>\n",
      "root/model/layers/12:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/12/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/12/self_attn:<o_proj,Linear>\n",
      "root/model/layers/12/self_attn:<v_proj,Linear>\n",
      "root/model/layers/12/self_attn:<k_proj,Linear>\n",
      "root/model/layers/12/self_attn:<q_proj,Linear>\n",
      "root/model/layers/12/mlp:<act_fn,SiLU>\n",
      "root/model/layers/12/mlp:<down_proj,Linear>\n",
      "root/model/layers/12/mlp:<up_proj,Linear>\n",
      "root/model/layers/12/mlp:<gate_proj,Linear>\n",
      "root/model/layers/13:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/13:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/13:<mlp,LlamaMLP>\n",
      "root/model/layers/13:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/13/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/13/self_attn:<o_proj,Linear>\n",
      "root/model/layers/13/self_attn:<v_proj,Linear>\n",
      "root/model/layers/13/self_attn:<k_proj,Linear>\n",
      "root/model/layers/13/self_attn:<q_proj,Linear>\n",
      "root/model/layers/13/mlp:<act_fn,SiLU>\n",
      "root/model/layers/13/mlp:<down_proj,Linear>\n",
      "root/model/layers/13/mlp:<up_proj,Linear>\n",
      "root/model/layers/13/mlp:<gate_proj,Linear>\n",
      "root/model/layers/14:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/14:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/14:<mlp,LlamaMLP>\n",
      "root/model/layers/14:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/14/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/14/self_attn:<o_proj,Linear>\n",
      "root/model/layers/14/self_attn:<v_proj,Linear>\n",
      "root/model/layers/14/self_attn:<k_proj,Linear>\n",
      "root/model/layers/14/self_attn:<q_proj,Linear>\n",
      "root/model/layers/14/mlp:<act_fn,SiLU>\n",
      "root/model/layers/14/mlp:<down_proj,Linear>\n",
      "root/model/layers/14/mlp:<up_proj,Linear>\n",
      "root/model/layers/14/mlp:<gate_proj,Linear>\n",
      "root/model/layers/15:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/15:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/15:<mlp,LlamaMLP>\n",
      "root/model/layers/15:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/15/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/15/self_attn:<o_proj,Linear>\n",
      "root/model/layers/15/self_attn:<v_proj,Linear>\n",
      "root/model/layers/15/self_attn:<k_proj,Linear>\n",
      "root/model/layers/15/self_attn:<q_proj,Linear>\n",
      "root/model/layers/15/mlp:<act_fn,SiLU>\n",
      "root/model/layers/15/mlp:<down_proj,Linear>\n",
      "root/model/layers/15/mlp:<up_proj,Linear>\n",
      "root/model/layers/15/mlp:<gate_proj,Linear>\n",
      "root/model/layers/16:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/16:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/16:<mlp,LlamaMLP>\n",
      "root/model/layers/16:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/16/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/16/self_attn:<o_proj,Linear>\n",
      "root/model/layers/16/self_attn:<v_proj,Linear>\n",
      "root/model/layers/16/self_attn:<k_proj,Linear>\n",
      "root/model/layers/16/self_attn:<q_proj,Linear>\n",
      "root/model/layers/16/mlp:<act_fn,SiLU>\n",
      "root/model/layers/16/mlp:<down_proj,Linear>\n",
      "root/model/layers/16/mlp:<up_proj,Linear>\n",
      "root/model/layers/16/mlp:<gate_proj,Linear>\n",
      "root/model/layers/17:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/17:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/17:<mlp,LlamaMLP>\n",
      "root/model/layers/17:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/17/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/17/self_attn:<o_proj,Linear>\n",
      "root/model/layers/17/self_attn:<v_proj,Linear>\n",
      "root/model/layers/17/self_attn:<k_proj,Linear>\n",
      "root/model/layers/17/self_attn:<q_proj,Linear>\n",
      "root/model/layers/17/mlp:<act_fn,SiLU>\n",
      "root/model/layers/17/mlp:<down_proj,Linear>\n",
      "root/model/layers/17/mlp:<up_proj,Linear>\n",
      "root/model/layers/17/mlp:<gate_proj,Linear>\n",
      "root/model/layers/18:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/18:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/18:<mlp,LlamaMLP>\n",
      "root/model/layers/18:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/18/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/18/self_attn:<o_proj,Linear>\n",
      "root/model/layers/18/self_attn:<v_proj,Linear>\n",
      "root/model/layers/18/self_attn:<k_proj,Linear>\n",
      "root/model/layers/18/self_attn:<q_proj,Linear>\n",
      "root/model/layers/18/mlp:<act_fn,SiLU>\n",
      "root/model/layers/18/mlp:<down_proj,Linear>\n",
      "root/model/layers/18/mlp:<up_proj,Linear>\n",
      "root/model/layers/18/mlp:<gate_proj,Linear>\n",
      "root/model/layers/19:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/19:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/19:<mlp,LlamaMLP>\n",
      "root/model/layers/19:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/19/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/19/self_attn:<o_proj,Linear>\n",
      "root/model/layers/19/self_attn:<v_proj,Linear>\n",
      "root/model/layers/19/self_attn:<k_proj,Linear>\n",
      "root/model/layers/19/self_attn:<q_proj,Linear>\n",
      "root/model/layers/19/mlp:<act_fn,SiLU>\n",
      "root/model/layers/19/mlp:<down_proj,Linear>\n",
      "root/model/layers/19/mlp:<up_proj,Linear>\n",
      "root/model/layers/19/mlp:<gate_proj,Linear>\n",
      "root/model/layers/20:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/20:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/20:<mlp,LlamaMLP>\n",
      "root/model/layers/20:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/20/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/20/self_attn:<o_proj,Linear>\n",
      "root/model/layers/20/self_attn:<v_proj,Linear>\n",
      "root/model/layers/20/self_attn:<k_proj,Linear>\n",
      "root/model/layers/20/self_attn:<q_proj,Linear>\n",
      "root/model/layers/20/mlp:<act_fn,SiLU>\n",
      "root/model/layers/20/mlp:<down_proj,Linear>\n",
      "root/model/layers/20/mlp:<up_proj,Linear>\n",
      "root/model/layers/20/mlp:<gate_proj,Linear>\n",
      "root/model/layers/21:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/21:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/21:<mlp,LlamaMLP>\n",
      "root/model/layers/21:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/21/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/21/self_attn:<o_proj,Linear>\n",
      "root/model/layers/21/self_attn:<v_proj,Linear>\n",
      "root/model/layers/21/self_attn:<k_proj,Linear>\n",
      "root/model/layers/21/self_attn:<q_proj,Linear>\n",
      "root/model/layers/21/mlp:<act_fn,SiLU>\n",
      "root/model/layers/21/mlp:<down_proj,Linear>\n",
      "root/model/layers/21/mlp:<up_proj,Linear>\n",
      "root/model/layers/21/mlp:<gate_proj,Linear>\n",
      "root/model/layers/22:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/22:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/22:<mlp,LlamaMLP>\n",
      "root/model/layers/22:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/22/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/22/self_attn:<o_proj,Linear>\n",
      "root/model/layers/22/self_attn:<v_proj,Linear>\n",
      "root/model/layers/22/self_attn:<k_proj,Linear>\n",
      "root/model/layers/22/self_attn:<q_proj,Linear>\n",
      "root/model/layers/22/mlp:<act_fn,SiLU>\n",
      "root/model/layers/22/mlp:<down_proj,Linear>\n",
      "root/model/layers/22/mlp:<up_proj,Linear>\n",
      "root/model/layers/22/mlp:<gate_proj,Linear>\n",
      "root/model/layers/23:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/23:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/23:<mlp,LlamaMLP>\n",
      "root/model/layers/23:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/23/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/23/self_attn:<o_proj,Linear>\n",
      "root/model/layers/23/self_attn:<v_proj,Linear>\n",
      "root/model/layers/23/self_attn:<k_proj,Linear>\n",
      "root/model/layers/23/self_attn:<q_proj,Linear>\n",
      "root/model/layers/23/mlp:<act_fn,SiLU>\n",
      "root/model/layers/23/mlp:<down_proj,Linear>\n",
      "root/model/layers/23/mlp:<up_proj,Linear>\n",
      "root/model/layers/23/mlp:<gate_proj,Linear>\n",
      "root/model/layers/24:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/24:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/24:<mlp,LlamaMLP>\n",
      "root/model/layers/24:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/24/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/24/self_attn:<o_proj,Linear>\n",
      "root/model/layers/24/self_attn:<v_proj,Linear>\n",
      "root/model/layers/24/self_attn:<k_proj,Linear>\n",
      "root/model/layers/24/self_attn:<q_proj,Linear>\n",
      "root/model/layers/24/mlp:<act_fn,SiLU>\n",
      "root/model/layers/24/mlp:<down_proj,Linear>\n",
      "root/model/layers/24/mlp:<up_proj,Linear>\n",
      "root/model/layers/24/mlp:<gate_proj,Linear>\n",
      "root/model/layers/25:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/25:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/25:<mlp,LlamaMLP>\n",
      "root/model/layers/25:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/25/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/25/self_attn:<o_proj,Linear>\n",
      "root/model/layers/25/self_attn:<v_proj,Linear>\n",
      "root/model/layers/25/self_attn:<k_proj,Linear>\n",
      "root/model/layers/25/self_attn:<q_proj,Linear>\n",
      "root/model/layers/25/mlp:<act_fn,SiLU>\n",
      "root/model/layers/25/mlp:<down_proj,Linear>\n",
      "root/model/layers/25/mlp:<up_proj,Linear>\n",
      "root/model/layers/25/mlp:<gate_proj,Linear>\n",
      "root/model/layers/26:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/26:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/26:<mlp,LlamaMLP>\n",
      "root/model/layers/26:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/26/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/26/self_attn:<o_proj,Linear>\n",
      "root/model/layers/26/self_attn:<v_proj,Linear>\n",
      "root/model/layers/26/self_attn:<k_proj,Linear>\n",
      "root/model/layers/26/self_attn:<q_proj,Linear>\n",
      "root/model/layers/26/mlp:<act_fn,SiLU>\n",
      "root/model/layers/26/mlp:<down_proj,Linear>\n",
      "root/model/layers/26/mlp:<up_proj,Linear>\n",
      "root/model/layers/26/mlp:<gate_proj,Linear>\n",
      "root/model/layers/27:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/27:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/27:<mlp,LlamaMLP>\n",
      "root/model/layers/27:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/27/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/27/self_attn:<o_proj,Linear>\n",
      "root/model/layers/27/self_attn:<v_proj,Linear>\n",
      "root/model/layers/27/self_attn:<k_proj,Linear>\n",
      "root/model/layers/27/self_attn:<q_proj,Linear>\n",
      "root/model/layers/27/mlp:<act_fn,SiLU>\n",
      "root/model/layers/27/mlp:<down_proj,Linear>\n",
      "root/model/layers/27/mlp:<up_proj,Linear>\n",
      "root/model/layers/27/mlp:<gate_proj,Linear>\n",
      "root/model/layers/28:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/28:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/28:<mlp,LlamaMLP>\n",
      "root/model/layers/28:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/28/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/28/self_attn:<o_proj,Linear>\n",
      "root/model/layers/28/self_attn:<v_proj,Linear>\n",
      "root/model/layers/28/self_attn:<k_proj,Linear>\n",
      "root/model/layers/28/self_attn:<q_proj,Linear>\n",
      "root/model/layers/28/mlp:<act_fn,SiLU>\n",
      "root/model/layers/28/mlp:<down_proj,Linear>\n",
      "root/model/layers/28/mlp:<up_proj,Linear>\n",
      "root/model/layers/28/mlp:<gate_proj,Linear>\n",
      "root/model/layers/29:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/29:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/29:<mlp,LlamaMLP>\n",
      "root/model/layers/29:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/29/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/29/self_attn:<o_proj,Linear>\n",
      "root/model/layers/29/self_attn:<v_proj,Linear>\n",
      "root/model/layers/29/self_attn:<k_proj,Linear>\n",
      "root/model/layers/29/self_attn:<q_proj,Linear>\n",
      "root/model/layers/29/mlp:<act_fn,SiLU>\n",
      "root/model/layers/29/mlp:<down_proj,Linear>\n",
      "root/model/layers/29/mlp:<up_proj,Linear>\n",
      "root/model/layers/29/mlp:<gate_proj,Linear>\n",
      "root/model/layers/30:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/30:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/30:<mlp,LlamaMLP>\n",
      "root/model/layers/30:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/30/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/30/self_attn:<o_proj,Linear>\n",
      "root/model/layers/30/self_attn:<v_proj,Linear>\n",
      "root/model/layers/30/self_attn:<k_proj,Linear>\n",
      "root/model/layers/30/self_attn:<q_proj,Linear>\n",
      "root/model/layers/30/mlp:<act_fn,SiLU>\n",
      "root/model/layers/30/mlp:<down_proj,Linear>\n",
      "root/model/layers/30/mlp:<up_proj,Linear>\n",
      "root/model/layers/30/mlp:<gate_proj,Linear>\n",
      "root/model/layers/31:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/31:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/31:<mlp,LlamaMLP>\n",
      "root/model/layers/31:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/31/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/31/self_attn:<o_proj,Linear>\n",
      "root/model/layers/31/self_attn:<v_proj,Linear>\n",
      "root/model/layers/31/self_attn:<k_proj,Linear>\n",
      "root/model/layers/31/self_attn:<q_proj,Linear>\n",
      "root/model/layers/31/mlp:<act_fn,SiLU>\n",
      "root/model/layers/31/mlp:<down_proj,Linear>\n",
      "root/model/layers/31/mlp:<up_proj,Linear>\n",
      "root/model/layers/31/mlp:<gate_proj,Linear>\n",
      "root/model/layers/32:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/32:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/32:<mlp,LlamaMLP>\n",
      "root/model/layers/32:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/32/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/32/self_attn:<o_proj,Linear>\n",
      "root/model/layers/32/self_attn:<v_proj,Linear>\n",
      "root/model/layers/32/self_attn:<k_proj,Linear>\n",
      "root/model/layers/32/self_attn:<q_proj,Linear>\n",
      "root/model/layers/32/mlp:<act_fn,SiLU>\n",
      "root/model/layers/32/mlp:<down_proj,Linear>\n",
      "root/model/layers/32/mlp:<up_proj,Linear>\n",
      "root/model/layers/32/mlp:<gate_proj,Linear>\n",
      "root/model/layers/33:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/33:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/33:<mlp,LlamaMLP>\n",
      "root/model/layers/33:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/33/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/33/self_attn:<o_proj,Linear>\n",
      "root/model/layers/33/self_attn:<v_proj,Linear>\n",
      "root/model/layers/33/self_attn:<k_proj,Linear>\n",
      "root/model/layers/33/self_attn:<q_proj,Linear>\n",
      "root/model/layers/33/mlp:<act_fn,SiLU>\n",
      "root/model/layers/33/mlp:<down_proj,Linear>\n",
      "root/model/layers/33/mlp:<up_proj,Linear>\n",
      "root/model/layers/33/mlp:<gate_proj,Linear>\n",
      "root/model/layers/34:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/34:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/34:<mlp,LlamaMLP>\n",
      "root/model/layers/34:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/34/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/34/self_attn:<o_proj,Linear>\n",
      "root/model/layers/34/self_attn:<v_proj,Linear>\n",
      "root/model/layers/34/self_attn:<k_proj,Linear>\n",
      "root/model/layers/34/self_attn:<q_proj,Linear>\n",
      "root/model/layers/34/mlp:<act_fn,SiLU>\n",
      "root/model/layers/34/mlp:<down_proj,Linear>\n",
      "root/model/layers/34/mlp:<up_proj,Linear>\n",
      "root/model/layers/34/mlp:<gate_proj,Linear>\n",
      "root/model/layers/35:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/35:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/35:<mlp,LlamaMLP>\n",
      "root/model/layers/35:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/35/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/35/self_attn:<o_proj,Linear>\n",
      "root/model/layers/35/self_attn:<v_proj,Linear>\n",
      "root/model/layers/35/self_attn:<k_proj,Linear>\n",
      "root/model/layers/35/self_attn:<q_proj,Linear>\n",
      "root/model/layers/35/mlp:<act_fn,SiLU>\n",
      "root/model/layers/35/mlp:<down_proj,Linear>\n",
      "root/model/layers/35/mlp:<up_proj,Linear>\n",
      "root/model/layers/35/mlp:<gate_proj,Linear>\n",
      "root/model/layers/36:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/36:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/36:<mlp,LlamaMLP>\n",
      "root/model/layers/36:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/36/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/36/self_attn:<o_proj,Linear>\n",
      "root/model/layers/36/self_attn:<v_proj,Linear>\n",
      "root/model/layers/36/self_attn:<k_proj,Linear>\n",
      "root/model/layers/36/self_attn:<q_proj,Linear>\n",
      "root/model/layers/36/mlp:<act_fn,SiLU>\n",
      "root/model/layers/36/mlp:<down_proj,Linear>\n",
      "root/model/layers/36/mlp:<up_proj,Linear>\n",
      "root/model/layers/36/mlp:<gate_proj,Linear>\n",
      "root/model/layers/37:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/37:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/37:<mlp,LlamaMLP>\n",
      "root/model/layers/37:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/37/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/37/self_attn:<o_proj,Linear>\n",
      "root/model/layers/37/self_attn:<v_proj,Linear>\n",
      "root/model/layers/37/self_attn:<k_proj,Linear>\n",
      "root/model/layers/37/self_attn:<q_proj,Linear>\n",
      "root/model/layers/37/mlp:<act_fn,SiLU>\n",
      "root/model/layers/37/mlp:<down_proj,Linear>\n",
      "root/model/layers/37/mlp:<up_proj,Linear>\n",
      "root/model/layers/37/mlp:<gate_proj,Linear>\n",
      "root/model/layers/38:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/38:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/38:<mlp,LlamaMLP>\n",
      "root/model/layers/38:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/38/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/38/self_attn:<o_proj,Linear>\n",
      "root/model/layers/38/self_attn:<v_proj,Linear>\n",
      "root/model/layers/38/self_attn:<k_proj,Linear>\n",
      "root/model/layers/38/self_attn:<q_proj,Linear>\n",
      "root/model/layers/38/mlp:<act_fn,SiLU>\n",
      "root/model/layers/38/mlp:<down_proj,Linear>\n",
      "root/model/layers/38/mlp:<up_proj,Linear>\n",
      "root/model/layers/38/mlp:<gate_proj,Linear>\n",
      "root/model/layers/39:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/39:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/39:<mlp,LlamaMLP>\n",
      "root/model/layers/39:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/39/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/39/self_attn:<o_proj,Linear>\n",
      "root/model/layers/39/self_attn:<v_proj,Linear>\n",
      "root/model/layers/39/self_attn:<k_proj,Linear>\n",
      "root/model/layers/39/self_attn:<q_proj,Linear>\n",
      "root/model/layers/39/mlp:<act_fn,SiLU>\n",
      "root/model/layers/39/mlp:<down_proj,Linear>\n",
      "root/model/layers/39/mlp:<up_proj,Linear>\n",
      "root/model/layers/39/mlp:<gate_proj,Linear>\n"
     ]
    }
   ],
   "source": [
    "# \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "# model_name = \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "# hg_token = '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'\n",
    "# hooked_llama = hooked_transformer(model_name, hg_token = hg_token)\n",
    "model_name = \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "hf_token = 'your token'\n",
    "hook_model = hooked_transformer(model_name, hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fa05e1-109d-4010-94f7-ab0e2c4138ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook of root/model/layers:<9,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<10,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<11,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<12,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<13,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<14,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<15,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<16,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<17,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<18,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<19,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<20,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<21,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<22,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<23,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<24,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<25,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<26,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<27,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<28,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<29,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<30,LlamaDecoderLayer> added.\n"
     ]
    }
   ],
   "source": [
    "#-3 :39\n",
    "#-4 :38\n",
    "#-10:28\n",
    "#-32:3add_hook_to_layer\n",
    "#hook_model.add_hook_for_each_layer_by_name('rep_layer_component_hook', 'input_layernorm', list(range(10,32)))\n",
    "hook_model.add_hook_to_layer('rep_layer_hook', list(range(9,31)))\n",
    "#hook_model.add_layer_hook(list(range(0,40)))\n",
    "hook_model.set_hook_parameters(mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bce57df-e721-488a-b346-5f6ba18cf395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1024\n",
      "Test data: 512\n"
     ]
    }
   ],
   "source": [
    "user_tag = \"USER:\"\n",
    "assistant_tag = \"ASSISTANT:\"\n",
    "data_path = 'data/facts_true_false.csv'\n",
    "dataset = honesty_function_dataset(data_path, hook_model.tokenizer, user_tag, assistant_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73feda4-34e2-4b3b-bb32-95c37c7a6389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9c2ff442654fe7aa44b9341f16be16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 47.44 GiB of which 196.75 MiB is free. Including non-PyTorch memory, this process has 47.24 GiB memory in use. Of the allocated memory 46.18 GiB is allocated by PyTorch, and 766.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m hook_model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, input_data_label \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mbatch_size])\n",
      "File \u001b[0;32m/home/cs/x/xxr230000/NLP_team_project/hooked_transformer.py:160\u001b[0m, in \u001b[0;36mhooked_transformer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddenstate_label\u001b[38;5;241m.\u001b[39mextend(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_data_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_data_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1190\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1191\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1192\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1193\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1194\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1195\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1196\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1197\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1198\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1199\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    990\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m         position_embeddings,\n\u001b[1;32m    998\u001b[0m     )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1001\u001b[0m         hidden_states,\n\u001b[1;32m   1002\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1003\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1004\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1005\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1006\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1007\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1008\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1009\u001b[0m     )\n\u001b[1;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:745\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    746\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:311\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    309\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    352\u001b[0m         ):\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 355\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    356\u001b[0m             module,\n\u001b[1;32m    357\u001b[0m             name,\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device,\n\u001b[1;32m    359\u001b[0m             value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    360\u001b[0m             fp16_statistics\u001b[38;5;241m=\u001b[39mfp16_statistics,\n\u001b[1;32m    361\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    362\u001b[0m         )\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 47.44 GiB of which 196.75 MiB is free. Including non-PyTorch memory, this process has 47.24 GiB memory in use. Of the allocated memory 46.18 GiB is allocated by PyTorch, and 766.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# batch process\n",
    "for i in tqdm(range(0, len(dataset['train']['data']), batch_size), desc=\"Inferencing\"):\n",
    "    encodings = hook_model.tokenizer(dataset['train']['data'][i:i+batch_size], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hook_model(input_ids=input_ids, attention_mask=attention_mask, input_data_label = dataset['train']['labels'][i:i+batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f259f-91a9-4069-908b-0f3140677833",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_model.calculate_hook_direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459704b-3347-4e5a-b0fd-ed0adfc153e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "hook_model.set_hook_parameters(mode = 'freeze')\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------No Control------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803bb4d-8f75-47ca-8b47-978f8b780f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "hook_model.set_hook_parameters(mode = 'control', control_intensity = 2.7)\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------With +honesty Control------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700770c-8600-481c-ac5b-0d93f0527fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "hook_model.set_hook_parameters(mode = 'control', control_intensity = -2.7)\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------With -honesty Control------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
