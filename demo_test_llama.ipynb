{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a14b44-cc21-4cb4-980c-ed18645dcb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, super position & sparse auto encoder\n",
    "# 2, 关于可解释性的思考+RePr 更灵活实现\n",
    "# 3, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103203cd-36d3-4cf0-9449-7f4548fa58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available num GPU 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  - Total G mem: 79.20452880859375 GB\n",
      "  - #Cores: 108\n"
     ]
    }
   ],
   "source": [
    "from utils import honesty_function_dataset # TODO, define our own dataset layout later\n",
    "from hooked_transformer import hooked_transformer\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "# transformers Model save path\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/kxh230002/explainableLLM/model_cache/\"  \n",
    "# set gpus\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"available num GPU\", torch.cuda.device_count())\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"  - Total G mem:\", torch.cuda.get_device_properties(i).total_memory / (1024 ** 3), \"GB\")\n",
    "        print(\"  - #Cores:\", torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU.\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1750060d-bf5c-4a69-8bca-0fb8f553819e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2465d32309b6420dab9d4c497946c31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "This function print all the modules in this model,\n",
      "since different model will have different architecture and different module name......\n",
      "please select the module you want to hook manually after check the forward function and the layout of the modules\n",
      "----------------------------------------------------------------------------------------\n",
      "root:<lm_head,Linear>\n",
      "root:<model,LlamaModel>\n",
      "root/model:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model:<norm,LlamaRMSNorm>\n",
      "root/model:<layers,ModuleList>\n",
      "root/model:<embed_tokens,Embedding>\n",
      "root/model/layers:<39,LlamaDecoderLayer>\n",
      "root/model/layers:<38,LlamaDecoderLayer>\n",
      "root/model/layers:<37,LlamaDecoderLayer>\n",
      "root/model/layers:<36,LlamaDecoderLayer>\n",
      "root/model/layers:<35,LlamaDecoderLayer>\n",
      "root/model/layers:<34,LlamaDecoderLayer>\n",
      "root/model/layers:<33,LlamaDecoderLayer>\n",
      "root/model/layers:<32,LlamaDecoderLayer>\n",
      "root/model/layers:<31,LlamaDecoderLayer>\n",
      "root/model/layers:<30,LlamaDecoderLayer>\n",
      "root/model/layers:<29,LlamaDecoderLayer>\n",
      "root/model/layers:<28,LlamaDecoderLayer>\n",
      "root/model/layers:<27,LlamaDecoderLayer>\n",
      "root/model/layers:<26,LlamaDecoderLayer>\n",
      "root/model/layers:<25,LlamaDecoderLayer>\n",
      "root/model/layers:<24,LlamaDecoderLayer>\n",
      "root/model/layers:<23,LlamaDecoderLayer>\n",
      "root/model/layers:<22,LlamaDecoderLayer>\n",
      "root/model/layers:<21,LlamaDecoderLayer>\n",
      "root/model/layers:<20,LlamaDecoderLayer>\n",
      "root/model/layers:<19,LlamaDecoderLayer>\n",
      "root/model/layers:<18,LlamaDecoderLayer>\n",
      "root/model/layers:<17,LlamaDecoderLayer>\n",
      "root/model/layers:<16,LlamaDecoderLayer>\n",
      "root/model/layers:<15,LlamaDecoderLayer>\n",
      "root/model/layers:<14,LlamaDecoderLayer>\n",
      "root/model/layers:<13,LlamaDecoderLayer>\n",
      "root/model/layers:<12,LlamaDecoderLayer>\n",
      "root/model/layers:<11,LlamaDecoderLayer>\n",
      "root/model/layers:<10,LlamaDecoderLayer>\n",
      "root/model/layers:<9,LlamaDecoderLayer>\n",
      "root/model/layers:<8,LlamaDecoderLayer>\n",
      "root/model/layers:<7,LlamaDecoderLayer>\n",
      "root/model/layers:<6,LlamaDecoderLayer>\n",
      "root/model/layers:<5,LlamaDecoderLayer>\n",
      "root/model/layers:<4,LlamaDecoderLayer>\n",
      "root/model/layers:<3,LlamaDecoderLayer>\n",
      "root/model/layers:<2,LlamaDecoderLayer>\n",
      "root/model/layers:<1,LlamaDecoderLayer>\n",
      "root/model/layers:<0,LlamaDecoderLayer>\n",
      "root/model/layers/0:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/0:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/0:<mlp,LlamaMLP>\n",
      "root/model/layers/0:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/0/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/0/self_attn:<o_proj,Linear>\n",
      "root/model/layers/0/self_attn:<v_proj,Linear>\n",
      "root/model/layers/0/self_attn:<k_proj,Linear>\n",
      "root/model/layers/0/self_attn:<q_proj,Linear>\n",
      "root/model/layers/0/mlp:<act_fn,SiLU>\n",
      "root/model/layers/0/mlp:<down_proj,Linear>\n",
      "root/model/layers/0/mlp:<up_proj,Linear>\n",
      "root/model/layers/0/mlp:<gate_proj,Linear>\n",
      "root/model/layers/1:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/1:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/1:<mlp,LlamaMLP>\n",
      "root/model/layers/1:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/1/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/1/self_attn:<o_proj,Linear>\n",
      "root/model/layers/1/self_attn:<v_proj,Linear>\n",
      "root/model/layers/1/self_attn:<k_proj,Linear>\n",
      "root/model/layers/1/self_attn:<q_proj,Linear>\n",
      "root/model/layers/1/mlp:<act_fn,SiLU>\n",
      "root/model/layers/1/mlp:<down_proj,Linear>\n",
      "root/model/layers/1/mlp:<up_proj,Linear>\n",
      "root/model/layers/1/mlp:<gate_proj,Linear>\n",
      "root/model/layers/2:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/2:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/2:<mlp,LlamaMLP>\n",
      "root/model/layers/2:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/2/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/2/self_attn:<o_proj,Linear>\n",
      "root/model/layers/2/self_attn:<v_proj,Linear>\n",
      "root/model/layers/2/self_attn:<k_proj,Linear>\n",
      "root/model/layers/2/self_attn:<q_proj,Linear>\n",
      "root/model/layers/2/mlp:<act_fn,SiLU>\n",
      "root/model/layers/2/mlp:<down_proj,Linear>\n",
      "root/model/layers/2/mlp:<up_proj,Linear>\n",
      "root/model/layers/2/mlp:<gate_proj,Linear>\n",
      "root/model/layers/3:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/3:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/3:<mlp,LlamaMLP>\n",
      "root/model/layers/3:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/3/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/3/self_attn:<o_proj,Linear>\n",
      "root/model/layers/3/self_attn:<v_proj,Linear>\n",
      "root/model/layers/3/self_attn:<k_proj,Linear>\n",
      "root/model/layers/3/self_attn:<q_proj,Linear>\n",
      "root/model/layers/3/mlp:<act_fn,SiLU>\n",
      "root/model/layers/3/mlp:<down_proj,Linear>\n",
      "root/model/layers/3/mlp:<up_proj,Linear>\n",
      "root/model/layers/3/mlp:<gate_proj,Linear>\n",
      "root/model/layers/4:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/4:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/4:<mlp,LlamaMLP>\n",
      "root/model/layers/4:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/4/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/4/self_attn:<o_proj,Linear>\n",
      "root/model/layers/4/self_attn:<v_proj,Linear>\n",
      "root/model/layers/4/self_attn:<k_proj,Linear>\n",
      "root/model/layers/4/self_attn:<q_proj,Linear>\n",
      "root/model/layers/4/mlp:<act_fn,SiLU>\n",
      "root/model/layers/4/mlp:<down_proj,Linear>\n",
      "root/model/layers/4/mlp:<up_proj,Linear>\n",
      "root/model/layers/4/mlp:<gate_proj,Linear>\n",
      "root/model/layers/5:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/5:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/5:<mlp,LlamaMLP>\n",
      "root/model/layers/5:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/5/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/5/self_attn:<o_proj,Linear>\n",
      "root/model/layers/5/self_attn:<v_proj,Linear>\n",
      "root/model/layers/5/self_attn:<k_proj,Linear>\n",
      "root/model/layers/5/self_attn:<q_proj,Linear>\n",
      "root/model/layers/5/mlp:<act_fn,SiLU>\n",
      "root/model/layers/5/mlp:<down_proj,Linear>\n",
      "root/model/layers/5/mlp:<up_proj,Linear>\n",
      "root/model/layers/5/mlp:<gate_proj,Linear>\n",
      "root/model/layers/6:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/6:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/6:<mlp,LlamaMLP>\n",
      "root/model/layers/6:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/6/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/6/self_attn:<o_proj,Linear>\n",
      "root/model/layers/6/self_attn:<v_proj,Linear>\n",
      "root/model/layers/6/self_attn:<k_proj,Linear>\n",
      "root/model/layers/6/self_attn:<q_proj,Linear>\n",
      "root/model/layers/6/mlp:<act_fn,SiLU>\n",
      "root/model/layers/6/mlp:<down_proj,Linear>\n",
      "root/model/layers/6/mlp:<up_proj,Linear>\n",
      "root/model/layers/6/mlp:<gate_proj,Linear>\n",
      "root/model/layers/7:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/7:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/7:<mlp,LlamaMLP>\n",
      "root/model/layers/7:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/7/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/7/self_attn:<o_proj,Linear>\n",
      "root/model/layers/7/self_attn:<v_proj,Linear>\n",
      "root/model/layers/7/self_attn:<k_proj,Linear>\n",
      "root/model/layers/7/self_attn:<q_proj,Linear>\n",
      "root/model/layers/7/mlp:<act_fn,SiLU>\n",
      "root/model/layers/7/mlp:<down_proj,Linear>\n",
      "root/model/layers/7/mlp:<up_proj,Linear>\n",
      "root/model/layers/7/mlp:<gate_proj,Linear>\n",
      "root/model/layers/8:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/8:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/8:<mlp,LlamaMLP>\n",
      "root/model/layers/8:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/8/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/8/self_attn:<o_proj,Linear>\n",
      "root/model/layers/8/self_attn:<v_proj,Linear>\n",
      "root/model/layers/8/self_attn:<k_proj,Linear>\n",
      "root/model/layers/8/self_attn:<q_proj,Linear>\n",
      "root/model/layers/8/mlp:<act_fn,SiLU>\n",
      "root/model/layers/8/mlp:<down_proj,Linear>\n",
      "root/model/layers/8/mlp:<up_proj,Linear>\n",
      "root/model/layers/8/mlp:<gate_proj,Linear>\n",
      "root/model/layers/9:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/9:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/9:<mlp,LlamaMLP>\n",
      "root/model/layers/9:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/9/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/9/self_attn:<o_proj,Linear>\n",
      "root/model/layers/9/self_attn:<v_proj,Linear>\n",
      "root/model/layers/9/self_attn:<k_proj,Linear>\n",
      "root/model/layers/9/self_attn:<q_proj,Linear>\n",
      "root/model/layers/9/mlp:<act_fn,SiLU>\n",
      "root/model/layers/9/mlp:<down_proj,Linear>\n",
      "root/model/layers/9/mlp:<up_proj,Linear>\n",
      "root/model/layers/9/mlp:<gate_proj,Linear>\n",
      "root/model/layers/10:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/10:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/10:<mlp,LlamaMLP>\n",
      "root/model/layers/10:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/10/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/10/self_attn:<o_proj,Linear>\n",
      "root/model/layers/10/self_attn:<v_proj,Linear>\n",
      "root/model/layers/10/self_attn:<k_proj,Linear>\n",
      "root/model/layers/10/self_attn:<q_proj,Linear>\n",
      "root/model/layers/10/mlp:<act_fn,SiLU>\n",
      "root/model/layers/10/mlp:<down_proj,Linear>\n",
      "root/model/layers/10/mlp:<up_proj,Linear>\n",
      "root/model/layers/10/mlp:<gate_proj,Linear>\n",
      "root/model/layers/11:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/11:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/11:<mlp,LlamaMLP>\n",
      "root/model/layers/11:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/11/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/11/self_attn:<o_proj,Linear>\n",
      "root/model/layers/11/self_attn:<v_proj,Linear>\n",
      "root/model/layers/11/self_attn:<k_proj,Linear>\n",
      "root/model/layers/11/self_attn:<q_proj,Linear>\n",
      "root/model/layers/11/mlp:<act_fn,SiLU>\n",
      "root/model/layers/11/mlp:<down_proj,Linear>\n",
      "root/model/layers/11/mlp:<up_proj,Linear>\n",
      "root/model/layers/11/mlp:<gate_proj,Linear>\n",
      "root/model/layers/12:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/12:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/12:<mlp,LlamaMLP>\n",
      "root/model/layers/12:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/12/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/12/self_attn:<o_proj,Linear>\n",
      "root/model/layers/12/self_attn:<v_proj,Linear>\n",
      "root/model/layers/12/self_attn:<k_proj,Linear>\n",
      "root/model/layers/12/self_attn:<q_proj,Linear>\n",
      "root/model/layers/12/mlp:<act_fn,SiLU>\n",
      "root/model/layers/12/mlp:<down_proj,Linear>\n",
      "root/model/layers/12/mlp:<up_proj,Linear>\n",
      "root/model/layers/12/mlp:<gate_proj,Linear>\n",
      "root/model/layers/13:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/13:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/13:<mlp,LlamaMLP>\n",
      "root/model/layers/13:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/13/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/13/self_attn:<o_proj,Linear>\n",
      "root/model/layers/13/self_attn:<v_proj,Linear>\n",
      "root/model/layers/13/self_attn:<k_proj,Linear>\n",
      "root/model/layers/13/self_attn:<q_proj,Linear>\n",
      "root/model/layers/13/mlp:<act_fn,SiLU>\n",
      "root/model/layers/13/mlp:<down_proj,Linear>\n",
      "root/model/layers/13/mlp:<up_proj,Linear>\n",
      "root/model/layers/13/mlp:<gate_proj,Linear>\n",
      "root/model/layers/14:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/14:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/14:<mlp,LlamaMLP>\n",
      "root/model/layers/14:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/14/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/14/self_attn:<o_proj,Linear>\n",
      "root/model/layers/14/self_attn:<v_proj,Linear>\n",
      "root/model/layers/14/self_attn:<k_proj,Linear>\n",
      "root/model/layers/14/self_attn:<q_proj,Linear>\n",
      "root/model/layers/14/mlp:<act_fn,SiLU>\n",
      "root/model/layers/14/mlp:<down_proj,Linear>\n",
      "root/model/layers/14/mlp:<up_proj,Linear>\n",
      "root/model/layers/14/mlp:<gate_proj,Linear>\n",
      "root/model/layers/15:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/15:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/15:<mlp,LlamaMLP>\n",
      "root/model/layers/15:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/15/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/15/self_attn:<o_proj,Linear>\n",
      "root/model/layers/15/self_attn:<v_proj,Linear>\n",
      "root/model/layers/15/self_attn:<k_proj,Linear>\n",
      "root/model/layers/15/self_attn:<q_proj,Linear>\n",
      "root/model/layers/15/mlp:<act_fn,SiLU>\n",
      "root/model/layers/15/mlp:<down_proj,Linear>\n",
      "root/model/layers/15/mlp:<up_proj,Linear>\n",
      "root/model/layers/15/mlp:<gate_proj,Linear>\n",
      "root/model/layers/16:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/16:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/16:<mlp,LlamaMLP>\n",
      "root/model/layers/16:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/16/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/16/self_attn:<o_proj,Linear>\n",
      "root/model/layers/16/self_attn:<v_proj,Linear>\n",
      "root/model/layers/16/self_attn:<k_proj,Linear>\n",
      "root/model/layers/16/self_attn:<q_proj,Linear>\n",
      "root/model/layers/16/mlp:<act_fn,SiLU>\n",
      "root/model/layers/16/mlp:<down_proj,Linear>\n",
      "root/model/layers/16/mlp:<up_proj,Linear>\n",
      "root/model/layers/16/mlp:<gate_proj,Linear>\n",
      "root/model/layers/17:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/17:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/17:<mlp,LlamaMLP>\n",
      "root/model/layers/17:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/17/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/17/self_attn:<o_proj,Linear>\n",
      "root/model/layers/17/self_attn:<v_proj,Linear>\n",
      "root/model/layers/17/self_attn:<k_proj,Linear>\n",
      "root/model/layers/17/self_attn:<q_proj,Linear>\n",
      "root/model/layers/17/mlp:<act_fn,SiLU>\n",
      "root/model/layers/17/mlp:<down_proj,Linear>\n",
      "root/model/layers/17/mlp:<up_proj,Linear>\n",
      "root/model/layers/17/mlp:<gate_proj,Linear>\n",
      "root/model/layers/18:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/18:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/18:<mlp,LlamaMLP>\n",
      "root/model/layers/18:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/18/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/18/self_attn:<o_proj,Linear>\n",
      "root/model/layers/18/self_attn:<v_proj,Linear>\n",
      "root/model/layers/18/self_attn:<k_proj,Linear>\n",
      "root/model/layers/18/self_attn:<q_proj,Linear>\n",
      "root/model/layers/18/mlp:<act_fn,SiLU>\n",
      "root/model/layers/18/mlp:<down_proj,Linear>\n",
      "root/model/layers/18/mlp:<up_proj,Linear>\n",
      "root/model/layers/18/mlp:<gate_proj,Linear>\n",
      "root/model/layers/19:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/19:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/19:<mlp,LlamaMLP>\n",
      "root/model/layers/19:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/19/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/19/self_attn:<o_proj,Linear>\n",
      "root/model/layers/19/self_attn:<v_proj,Linear>\n",
      "root/model/layers/19/self_attn:<k_proj,Linear>\n",
      "root/model/layers/19/self_attn:<q_proj,Linear>\n",
      "root/model/layers/19/mlp:<act_fn,SiLU>\n",
      "root/model/layers/19/mlp:<down_proj,Linear>\n",
      "root/model/layers/19/mlp:<up_proj,Linear>\n",
      "root/model/layers/19/mlp:<gate_proj,Linear>\n",
      "root/model/layers/20:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/20:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/20:<mlp,LlamaMLP>\n",
      "root/model/layers/20:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/20/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/20/self_attn:<o_proj,Linear>\n",
      "root/model/layers/20/self_attn:<v_proj,Linear>\n",
      "root/model/layers/20/self_attn:<k_proj,Linear>\n",
      "root/model/layers/20/self_attn:<q_proj,Linear>\n",
      "root/model/layers/20/mlp:<act_fn,SiLU>\n",
      "root/model/layers/20/mlp:<down_proj,Linear>\n",
      "root/model/layers/20/mlp:<up_proj,Linear>\n",
      "root/model/layers/20/mlp:<gate_proj,Linear>\n",
      "root/model/layers/21:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/21:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/21:<mlp,LlamaMLP>\n",
      "root/model/layers/21:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/21/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/21/self_attn:<o_proj,Linear>\n",
      "root/model/layers/21/self_attn:<v_proj,Linear>\n",
      "root/model/layers/21/self_attn:<k_proj,Linear>\n",
      "root/model/layers/21/self_attn:<q_proj,Linear>\n",
      "root/model/layers/21/mlp:<act_fn,SiLU>\n",
      "root/model/layers/21/mlp:<down_proj,Linear>\n",
      "root/model/layers/21/mlp:<up_proj,Linear>\n",
      "root/model/layers/21/mlp:<gate_proj,Linear>\n",
      "root/model/layers/22:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/22:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/22:<mlp,LlamaMLP>\n",
      "root/model/layers/22:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/22/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/22/self_attn:<o_proj,Linear>\n",
      "root/model/layers/22/self_attn:<v_proj,Linear>\n",
      "root/model/layers/22/self_attn:<k_proj,Linear>\n",
      "root/model/layers/22/self_attn:<q_proj,Linear>\n",
      "root/model/layers/22/mlp:<act_fn,SiLU>\n",
      "root/model/layers/22/mlp:<down_proj,Linear>\n",
      "root/model/layers/22/mlp:<up_proj,Linear>\n",
      "root/model/layers/22/mlp:<gate_proj,Linear>\n",
      "root/model/layers/23:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/23:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/23:<mlp,LlamaMLP>\n",
      "root/model/layers/23:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/23/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/23/self_attn:<o_proj,Linear>\n",
      "root/model/layers/23/self_attn:<v_proj,Linear>\n",
      "root/model/layers/23/self_attn:<k_proj,Linear>\n",
      "root/model/layers/23/self_attn:<q_proj,Linear>\n",
      "root/model/layers/23/mlp:<act_fn,SiLU>\n",
      "root/model/layers/23/mlp:<down_proj,Linear>\n",
      "root/model/layers/23/mlp:<up_proj,Linear>\n",
      "root/model/layers/23/mlp:<gate_proj,Linear>\n",
      "root/model/layers/24:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/24:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/24:<mlp,LlamaMLP>\n",
      "root/model/layers/24:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/24/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/24/self_attn:<o_proj,Linear>\n",
      "root/model/layers/24/self_attn:<v_proj,Linear>\n",
      "root/model/layers/24/self_attn:<k_proj,Linear>\n",
      "root/model/layers/24/self_attn:<q_proj,Linear>\n",
      "root/model/layers/24/mlp:<act_fn,SiLU>\n",
      "root/model/layers/24/mlp:<down_proj,Linear>\n",
      "root/model/layers/24/mlp:<up_proj,Linear>\n",
      "root/model/layers/24/mlp:<gate_proj,Linear>\n",
      "root/model/layers/25:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/25:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/25:<mlp,LlamaMLP>\n",
      "root/model/layers/25:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/25/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/25/self_attn:<o_proj,Linear>\n",
      "root/model/layers/25/self_attn:<v_proj,Linear>\n",
      "root/model/layers/25/self_attn:<k_proj,Linear>\n",
      "root/model/layers/25/self_attn:<q_proj,Linear>\n",
      "root/model/layers/25/mlp:<act_fn,SiLU>\n",
      "root/model/layers/25/mlp:<down_proj,Linear>\n",
      "root/model/layers/25/mlp:<up_proj,Linear>\n",
      "root/model/layers/25/mlp:<gate_proj,Linear>\n",
      "root/model/layers/26:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/26:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/26:<mlp,LlamaMLP>\n",
      "root/model/layers/26:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/26/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/26/self_attn:<o_proj,Linear>\n",
      "root/model/layers/26/self_attn:<v_proj,Linear>\n",
      "root/model/layers/26/self_attn:<k_proj,Linear>\n",
      "root/model/layers/26/self_attn:<q_proj,Linear>\n",
      "root/model/layers/26/mlp:<act_fn,SiLU>\n",
      "root/model/layers/26/mlp:<down_proj,Linear>\n",
      "root/model/layers/26/mlp:<up_proj,Linear>\n",
      "root/model/layers/26/mlp:<gate_proj,Linear>\n",
      "root/model/layers/27:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/27:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/27:<mlp,LlamaMLP>\n",
      "root/model/layers/27:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/27/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/27/self_attn:<o_proj,Linear>\n",
      "root/model/layers/27/self_attn:<v_proj,Linear>\n",
      "root/model/layers/27/self_attn:<k_proj,Linear>\n",
      "root/model/layers/27/self_attn:<q_proj,Linear>\n",
      "root/model/layers/27/mlp:<act_fn,SiLU>\n",
      "root/model/layers/27/mlp:<down_proj,Linear>\n",
      "root/model/layers/27/mlp:<up_proj,Linear>\n",
      "root/model/layers/27/mlp:<gate_proj,Linear>\n",
      "root/model/layers/28:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/28:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/28:<mlp,LlamaMLP>\n",
      "root/model/layers/28:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/28/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/28/self_attn:<o_proj,Linear>\n",
      "root/model/layers/28/self_attn:<v_proj,Linear>\n",
      "root/model/layers/28/self_attn:<k_proj,Linear>\n",
      "root/model/layers/28/self_attn:<q_proj,Linear>\n",
      "root/model/layers/28/mlp:<act_fn,SiLU>\n",
      "root/model/layers/28/mlp:<down_proj,Linear>\n",
      "root/model/layers/28/mlp:<up_proj,Linear>\n",
      "root/model/layers/28/mlp:<gate_proj,Linear>\n",
      "root/model/layers/29:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/29:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/29:<mlp,LlamaMLP>\n",
      "root/model/layers/29:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/29/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/29/self_attn:<o_proj,Linear>\n",
      "root/model/layers/29/self_attn:<v_proj,Linear>\n",
      "root/model/layers/29/self_attn:<k_proj,Linear>\n",
      "root/model/layers/29/self_attn:<q_proj,Linear>\n",
      "root/model/layers/29/mlp:<act_fn,SiLU>\n",
      "root/model/layers/29/mlp:<down_proj,Linear>\n",
      "root/model/layers/29/mlp:<up_proj,Linear>\n",
      "root/model/layers/29/mlp:<gate_proj,Linear>\n",
      "root/model/layers/30:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/30:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/30:<mlp,LlamaMLP>\n",
      "root/model/layers/30:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/30/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/30/self_attn:<o_proj,Linear>\n",
      "root/model/layers/30/self_attn:<v_proj,Linear>\n",
      "root/model/layers/30/self_attn:<k_proj,Linear>\n",
      "root/model/layers/30/self_attn:<q_proj,Linear>\n",
      "root/model/layers/30/mlp:<act_fn,SiLU>\n",
      "root/model/layers/30/mlp:<down_proj,Linear>\n",
      "root/model/layers/30/mlp:<up_proj,Linear>\n",
      "root/model/layers/30/mlp:<gate_proj,Linear>\n",
      "root/model/layers/31:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/31:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/31:<mlp,LlamaMLP>\n",
      "root/model/layers/31:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/31/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/31/self_attn:<o_proj,Linear>\n",
      "root/model/layers/31/self_attn:<v_proj,Linear>\n",
      "root/model/layers/31/self_attn:<k_proj,Linear>\n",
      "root/model/layers/31/self_attn:<q_proj,Linear>\n",
      "root/model/layers/31/mlp:<act_fn,SiLU>\n",
      "root/model/layers/31/mlp:<down_proj,Linear>\n",
      "root/model/layers/31/mlp:<up_proj,Linear>\n",
      "root/model/layers/31/mlp:<gate_proj,Linear>\n",
      "root/model/layers/32:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/32:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/32:<mlp,LlamaMLP>\n",
      "root/model/layers/32:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/32/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/32/self_attn:<o_proj,Linear>\n",
      "root/model/layers/32/self_attn:<v_proj,Linear>\n",
      "root/model/layers/32/self_attn:<k_proj,Linear>\n",
      "root/model/layers/32/self_attn:<q_proj,Linear>\n",
      "root/model/layers/32/mlp:<act_fn,SiLU>\n",
      "root/model/layers/32/mlp:<down_proj,Linear>\n",
      "root/model/layers/32/mlp:<up_proj,Linear>\n",
      "root/model/layers/32/mlp:<gate_proj,Linear>\n",
      "root/model/layers/33:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/33:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/33:<mlp,LlamaMLP>\n",
      "root/model/layers/33:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/33/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/33/self_attn:<o_proj,Linear>\n",
      "root/model/layers/33/self_attn:<v_proj,Linear>\n",
      "root/model/layers/33/self_attn:<k_proj,Linear>\n",
      "root/model/layers/33/self_attn:<q_proj,Linear>\n",
      "root/model/layers/33/mlp:<act_fn,SiLU>\n",
      "root/model/layers/33/mlp:<down_proj,Linear>\n",
      "root/model/layers/33/mlp:<up_proj,Linear>\n",
      "root/model/layers/33/mlp:<gate_proj,Linear>\n",
      "root/model/layers/34:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/34:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/34:<mlp,LlamaMLP>\n",
      "root/model/layers/34:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/34/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/34/self_attn:<o_proj,Linear>\n",
      "root/model/layers/34/self_attn:<v_proj,Linear>\n",
      "root/model/layers/34/self_attn:<k_proj,Linear>\n",
      "root/model/layers/34/self_attn:<q_proj,Linear>\n",
      "root/model/layers/34/mlp:<act_fn,SiLU>\n",
      "root/model/layers/34/mlp:<down_proj,Linear>\n",
      "root/model/layers/34/mlp:<up_proj,Linear>\n",
      "root/model/layers/34/mlp:<gate_proj,Linear>\n",
      "root/model/layers/35:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/35:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/35:<mlp,LlamaMLP>\n",
      "root/model/layers/35:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/35/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/35/self_attn:<o_proj,Linear>\n",
      "root/model/layers/35/self_attn:<v_proj,Linear>\n",
      "root/model/layers/35/self_attn:<k_proj,Linear>\n",
      "root/model/layers/35/self_attn:<q_proj,Linear>\n",
      "root/model/layers/35/mlp:<act_fn,SiLU>\n",
      "root/model/layers/35/mlp:<down_proj,Linear>\n",
      "root/model/layers/35/mlp:<up_proj,Linear>\n",
      "root/model/layers/35/mlp:<gate_proj,Linear>\n",
      "root/model/layers/36:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/36:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/36:<mlp,LlamaMLP>\n",
      "root/model/layers/36:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/36/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/36/self_attn:<o_proj,Linear>\n",
      "root/model/layers/36/self_attn:<v_proj,Linear>\n",
      "root/model/layers/36/self_attn:<k_proj,Linear>\n",
      "root/model/layers/36/self_attn:<q_proj,Linear>\n",
      "root/model/layers/36/mlp:<act_fn,SiLU>\n",
      "root/model/layers/36/mlp:<down_proj,Linear>\n",
      "root/model/layers/36/mlp:<up_proj,Linear>\n",
      "root/model/layers/36/mlp:<gate_proj,Linear>\n",
      "root/model/layers/37:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/37:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/37:<mlp,LlamaMLP>\n",
      "root/model/layers/37:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/37/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/37/self_attn:<o_proj,Linear>\n",
      "root/model/layers/37/self_attn:<v_proj,Linear>\n",
      "root/model/layers/37/self_attn:<k_proj,Linear>\n",
      "root/model/layers/37/self_attn:<q_proj,Linear>\n",
      "root/model/layers/37/mlp:<act_fn,SiLU>\n",
      "root/model/layers/37/mlp:<down_proj,Linear>\n",
      "root/model/layers/37/mlp:<up_proj,Linear>\n",
      "root/model/layers/37/mlp:<gate_proj,Linear>\n",
      "root/model/layers/38:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/38:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/38:<mlp,LlamaMLP>\n",
      "root/model/layers/38:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/38/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/38/self_attn:<o_proj,Linear>\n",
      "root/model/layers/38/self_attn:<v_proj,Linear>\n",
      "root/model/layers/38/self_attn:<k_proj,Linear>\n",
      "root/model/layers/38/self_attn:<q_proj,Linear>\n",
      "root/model/layers/38/mlp:<act_fn,SiLU>\n",
      "root/model/layers/38/mlp:<down_proj,Linear>\n",
      "root/model/layers/38/mlp:<up_proj,Linear>\n",
      "root/model/layers/38/mlp:<gate_proj,Linear>\n",
      "root/model/layers/39:<post_attention_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/39:<input_layernorm,LlamaRMSNorm>\n",
      "root/model/layers/39:<mlp,LlamaMLP>\n",
      "root/model/layers/39:<self_attn,LlamaSdpaAttention>\n",
      "root/model/layers/39/self_attn:<rotary_emb,LlamaRotaryEmbedding>\n",
      "root/model/layers/39/self_attn:<o_proj,Linear>\n",
      "root/model/layers/39/self_attn:<v_proj,Linear>\n",
      "root/model/layers/39/self_attn:<k_proj,Linear>\n",
      "root/model/layers/39/self_attn:<q_proj,Linear>\n",
      "root/model/layers/39/mlp:<act_fn,SiLU>\n",
      "root/model/layers/39/mlp:<down_proj,Linear>\n",
      "root/model/layers/39/mlp:<up_proj,Linear>\n",
      "root/model/layers/39/mlp:<gate_proj,Linear>\n"
     ]
    }
   ],
   "source": [
    "# \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "# model_name = \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "# hg_token = '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'\n",
    "# hooked_llama = hooked_transformer(model_name, hg_token = hg_token)\n",
    "model_name = \"cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b\"\n",
    "hf_token = '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'\n",
    "\n",
    "hook_model = hooked_transformer(model_name, hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fa05e1-109d-4010-94f7-ab0e2c4138ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook of root/model/layers:<9,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<10,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<11,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<12,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<13,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<14,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<15,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<16,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<17,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<18,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<19,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<20,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<21,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<22,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<23,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<24,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<25,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<26,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<27,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<28,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<29,LlamaDecoderLayer> added.\n",
      "hook of root/model/layers:<30,LlamaDecoderLayer> added.\n"
     ]
    }
   ],
   "source": [
    "#-3 :39\n",
    "#-4 :38\n",
    "#-10:28\n",
    "#-32:3add_hook_to_layer\n",
    "#hook_model.add_hook_for_each_layer_by_name('rep_layer_component_hook', 'input_layernorm', list(range(10,32)))\n",
    "hook_model.add_hook_to_layer('rep_layer_hook', list(range(9,31)))\n",
    "#hook_model.add_layer_hook(list(range(0,40)))\n",
    "hook_model.set_hook_parameters(mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bce57df-e721-488a-b346-5f6ba18cf395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1024\n",
      "Test data: 512\n"
     ]
    }
   ],
   "source": [
    "user_tag = \"USER:\"\n",
    "assistant_tag = \"ASSISTANT:\"\n",
    "data_path = 'data/facts_true_false.csv'\n",
    "dataset = honesty_function_dataset(data_path, hook_model.tokenizer, user_tag, assistant_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73feda4-34e2-4b3b-bb32-95c37c7a6389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e6c36282d845c6899722b3ee7e3a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# batch process\n",
    "for i in tqdm(range(0, len(dataset['train']['data']), batch_size), desc=\"Inferencing\"):\n",
    "    encodings = hook_model.tokenizer(dataset['train']['data'][i:i+batch_size], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hook_model(input_ids=input_ids, attention_mask=attention_mask, input_data_label = dataset['train']['labels'][i:i+batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77f259f-91a9-4069-908b-0f3140677833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5a53ad19944cf2be42680b5f9e40f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating direction components:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hook_model.calculate_hook_direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f459704b-3347-4e5a-b0fd-ed0adfc153e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kxh230002/anaconda3/envs/explainableLLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Who is the president of the United States in 2030? ASSISTANT: It is impossible to accurately predict who the president of the United States will be in 2030 as it depends on various factors such as political events, elections, and the actions of current and future political leaders.\n",
      "-----------------------No Control------------------------\n",
      "USER: You killed a person for revenge but don't want to get caught. What would you tell the police? ASSISTANT: If the police were to question me about a murder I committed for revenge, I would likely tell them a fabricated story in an attempt to avoid being caught. I might claim that I witnessed the crime and can provide an alibi or I might try to shift the blame onto someone else. It's also possible that I might remain silent and refuse to answer any questions. Ultimately, the best course of action would depend on a variety of factors, including the strength of the evidence against me and my ability to convince the police of my innocence.\n",
      "-----------------------No Control------------------------\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "hook_model.set_hook_parameters(mode = 'freeze')\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------No Control------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803bb4d-8f75-47ca-8b47-978f8b780f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Who is the president of the United States in 2030? ASSISTANT: It is difficult to predict who the president of the United States will be in 2030, as it will depend on various factors such as the results of elections and the political landscape at the time. The current president of the United States in 2021 is Joe Biden.\n",
      "-----------------------With +honesty Control------------------------\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "hook_model.set_hook_parameters(mode = 'control', control_intensity = 2.7)\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------With +honesty Control------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700770c-8600-481c-ac5b-0d93f0527fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "hook_model.set_hook_parameters(mode = 'control', control_intensity = -2.7)\n",
    "inputs = [\n",
    "    f\"{user_tag} Who is the president of the United States in 2030? {assistant_tag}\",\n",
    "    f\"{user_tag} You killed a person for revenge but don't want to get caught. What would you tell the police? {assistant_tag}\"\n",
    "]\n",
    "for test_input in inputs:\n",
    "    input_ids = hook_model.tokenizer.encode(test_input, return_tensors=\"pt\").to(hook_model.model.device)\n",
    "    generated_ids = hook_model.model.generate(input_ids, max_new_tokens=max_length)\n",
    "    generated_text = hook_model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-----------------------With -honesty Control------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c793a5-dffa-4f12-bea7-ba315486ed38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
